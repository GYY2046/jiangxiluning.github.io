---
layout: post
comments: true
categories: 机器学习
title: "Bias Vs. Variance"
---
# Bias
- Symptom
$ J_{cv}(\theta)  -1 $ and $ J_{train}(\theta) $ are high.
- Prescription
1. Getting additional features
2. Adding polynomial features ($x_{1}^{2}, x_{2}^{2}, x_1x_2, etc $)
3. Decreasing $\lambda $

# Variance

- Symptom
$ J_{cv}(\theta) \gg J_{train}(\theta) $ and $J_{train}(\theta) $ is low.
- Prescription

  1. Getting more training samples
  2. Getting rid of some features
  3. Increasing $\lambda$



<div style="text-align:center"><img src ="http://n-moving.com/wp-content/uploads/2016/07/9048de55-a5a3-4deb-a645-60cd3629d5e3.png" /></div>

# Regularization
Very big $\lambda \rightarrow $ Bias(underfiiting)
very small $\lambda \rightarrow $ Variance(overfilling)

$\lambda $ selection : use the same training set and select the lambda that leads to the smallest CV Error and to check the Test Error.

<div style="text-align:center"><img src ="http://n-moving.com/wp-content/uploads/2016/07/e0d19705-0d6b-44a1-9e77-d660788cc76a.png" /></div>

# Learning Curve

- High Bias
$J_{train}(\theta) $ is close to $J_{cv}(\theta) $.
**Getting more data is useless!**

<div style="text-align:center"><img src ="http://n-moving.com/wp-content/uploads/2016/07/a0bbb206-472b-410c-bff6-b861a70df9ef.png" /></div>


- High Variance
There is a gap between $J_{train}(\theta) $ and $J_{cv}(\theta) $.
**Getting more data may give a better result.**
<div style="text-align:center"><img src ="http://n-moving.com/wp-content/uploads/2016/07/5bb1cf3b-0085-4f65-a77b-e03e6f710072.png" /></div>


# Neural networks and overfitting
- Using "large" neural network with good regularization to address overfiting is usually better than "small" neural network, but the computation cost is more expensive.
