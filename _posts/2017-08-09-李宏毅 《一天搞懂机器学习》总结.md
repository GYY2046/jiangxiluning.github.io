---
layout: post
comments: true
categories: 深度学习
title: "李宏毅 《一天搞懂机器学习》总结"
---


# 深度学习的 Tips

## 获得比较好的训练结果

### 损失函数
- 使用 softmax 输出层时选择交叉熵

### Mini-batch（加速训练和更好的性能）
- 将训练数据分为 N 个小批次，重复训练 M 次

### 激活函数
- ReLU（Rectified Linear Unit）
- Maxout (ReLU is a special cases of Maxout)
  ReLU 是 Maxout 的特例， Maxout 可以学习到多段区间

### 自适应学习率
- 每次按一定比率减少学习率 $\eta^t = \eta/\sqrt{t+1}$
- Adagrad
  $$ w \leftarrow w - \eta_w \partial{L}/\partial{w} $$
$$\eta_w = \frac{\eta}{ \sqrt{\sum_{i=0}^{t}{(\mathscr{g}^i)^2}}}  $$
，$\mathscr{g}^i$ 是第 i 次更新 $\partial{L}/\partial{w} $ 的值

### 动量
- Adam 最优化算法

## 获得比较好的测试结果

### Early Stopping
过拟合导致测试误差较大，可以采取提前停止的方法

### Weight Decay
衰减率 $\lambda$， $ w \leftarrow (1-\lambda) w - \eta_w \partial{L}/\partial{w}$

### Dropout
训练时每个神经元都有 p% 的概率被剔除， 测试时却不剔除

## Network Structure

### CNN
- 卷积层
- Pooling

### RNN
- 1 of N 编码
- 1 of N+1 编码
- word hashing
- GRU
- Clockwise RNN
- Structurally Constrained Recurrent Network （SCRN）

### Ultra Deep Network

### Attention Model

### 强化学习
- Alpha Go
- 自动驾驶
- 自动飞行

### 非监督学习
- 自动编码器
- GAN（生成对抗网络）
- VAE（可变自动编码器）
